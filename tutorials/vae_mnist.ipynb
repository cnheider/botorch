{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE MNIST Example: BO in a Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem setup\n",
    "\n",
    "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
    "\n",
    "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
    "\\longrightarrow \\text{score}.$$\n",
    "\n",
    "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next instantiate the CNN for digit recognition and load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "bento_obj_id": "140231048235840"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = Net().to(device)\n",
    "cnn_model.load_state_dict(torch.load(\"../pretrained_models/mnist_cnn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "bento_obj_id": "140231071066224"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "vae_model.load_state_dict(torch.load(\"../pretrained_models/mnist_vae.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y):\n",
    "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
    "    centered at the digit '3'.\n",
    "    \"\"\"\n",
    "    return torch.exp(-2*(y-3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_image_recognition(x):\n",
    "    \"\"\"The input x is an image and an expected score based on the CNN classifier and\n",
    "    the scoring function is returned.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.exp(cnn_model(x))  # b x 10\n",
    "        scores = score(torch.arange(10, device=device, dtype=dtype)).expand(probs.shape)\n",
    "    return (probs * scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization and initial random batch\n",
    "\n",
    "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "\n",
    "bounds = torch.tensor([[-6.0]*20, [6.0]*20], device=device, dtype=dtype)\n",
    "\n",
    "def initialize_model(n=5):\n",
    "    # generate training data  \n",
    "    train_x = (bounds[1] - bounds[0]) * torch.rand(n, 20, device=device, dtype=dtype) + bounds[0]\n",
    "    train_obj = score_image_recognition(decode(train_x))\n",
    "    best_observed_value = train_obj.max().item()\n",
    "    \n",
    "    # define models for objective and constraint\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "    model = model.to(device=device, dtype=dtype)\n",
    "    \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll = mll.to(device=device, dtype=dtype)\n",
    "    \n",
    "    return train_x, train_obj, mll, model, best_observed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from botorch.optim import joint_optimize\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation\"\"\"\n",
    "    \n",
    "    # optimize\n",
    "    candidates = joint_optimize(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=10,\n",
    "        raw_samples=100,\n",
    "        options={\"simple_init\": True, \"maxiter\": 200},\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x = candidates.detach()\n",
    "    new_obj = score_image_recognition(decode(new_x))\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Bayesian Optimization loop with qEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2000` samples. We also initialize the model with 5 randomly drawn points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.acquisition.sampler import SobolQMCNormalSampler\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N_BATCH = 75\n",
    "MC_SAMPLES = 2000\n",
    "best_observed = []\n",
    "\n",
    "# call helper function to initialize model\n",
    "train_x, train_obj, mll, model, best_value = initialize_model(n=5)\n",
    "best_observed.append(best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the BO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running BO..........................................................................."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"\\nRunning BO\", end='')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(N_BATCH):    \n",
    "\n",
    "    # fit the model\n",
    "    fit_gpytorch_model(mll, options={\"maxiter\": 100})\n",
    "\n",
    "    # define the qNEI acquisition module using a QMC sampler\n",
    "    qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=seed)\n",
    "    qEI = qExpectedImprovement(model=model, sampler=qmc_sampler, best_f=best_value)\n",
    "\n",
    "    # optimize and get new observation\n",
    "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
    "\n",
    "    # update training points\n",
    "    train_x = torch.cat((train_x, new_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    # update progress\n",
    "    best_value = score_image_recognition(decode(train_x)).max().item()\n",
    "    best_observed.append(best_value)\n",
    "\n",
    "    # reinitialize the model so it is ready for fitting on next iteration\n",
    "    model.reinitialize(train_X=train_x, train_Y=train_obj)\n",
    "    \n",
    "    print(\".\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAywAAACQCAYAAADqbu8OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "bento_obj_id": "140230448870792"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
    "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
    "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
    "\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    b = torch.argmax(score_image_recognition(decode(train_x[:inds[i],:])),dim=0)\n",
    "    img = decode(train_x[b].view(1,-1)).squeeze().cpu()\n",
    "    ax.imshow(img, alpha=0.8, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "ae_lazarus (local)",
   "language": "python",
   "name": "ae_lazarus_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
